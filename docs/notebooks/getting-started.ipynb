{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "living-tablet",
   "metadata": {},
   "source": [
    "## A quick-start guide on using `fqlag` package\n",
    "\n",
    "`fqlag` is written in pure python. The base calculations are implemented using `numpy`'s vectorizations, the matrix inversion, which is typically the slowest part of calculating the likelihood function uses numpy/scipy which run in multiple cpu's by default.\n",
    "\n",
    "This documents gives a simple example of calculating the periodogram and delays from simulated light curves, found in `data.lc`.\n",
    "\n",
    "Other usage examples can be found in the `test.py` file in the main repository directory.\n",
    "\n",
    "Individual functions and classes have some documentations too.\n",
    "\n",
    "\n",
    "### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fqlag if not already installed\n",
    "!pip install fqlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fqlag\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change some settings in the plot to make them clear\n",
    "plt.rcParams.update({\n",
    "    'font.size': 16, \n",
    "    'font.family': 'monospace'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-protocol",
   "metadata": {},
   "source": [
    "Uncomment the following if you want to use new simulated light curves. The following uses the code in `test.py`, which relies on [`aztools`](https://github.com/zoghbi-a/aztools).\n",
    "\n",
    "The Following tutorial uses simlated light curves in `data.lc`\n",
    "The file has two light curves (continuously-sampled), with the file having 5 columns: \n",
    "\n",
    "`time`, `rate_1`, `rateErr_1`, `rate_2`, `rateErr_2`\n",
    "\n",
    "There is a phase delay of 1 radian between the two light curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.insert(0, '.')\n",
    "# import test\n",
    "\n",
    "# s = test.simulate_lc(1024, 1.0, 500, 1, bkp = [1e-4, -1, -2, 6e-3], phase=True)\n",
    "# s = np.array(s[:-1])\n",
    "# txt = '\\n'.join([' '.join(['%4.4g'%s[j,i] for j in range(s.shape[0])]) for i in range(s.shape[1])])\n",
    "# with open('data.lc', 'w') as fp: fp.write(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-dylan",
   "metadata": {},
   "source": [
    "### Read and plot the light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read light curves\n",
    "data_raw = np.loadtxt('data.lc')\n",
    "\n",
    "# add some gaps #\n",
    "np.random.seed(2334)\n",
    "idx = np.arange(s.shape[1])\n",
    "prob = np.cos(0.6*idx)**2\n",
    "idxGap = np.sort(np.random.choice(idx, np.int(len(idx)/2), replace=False, p = prob/prob.sum()))\n",
    "data = data_raw[idxGap, :]\n",
    "\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "_ = plt.errorbar(data[:,0], data[:,1], data[:,2], fmt='-', ms=4, lw=1, alpha=0.6)\n",
    "_ = plt.errorbar(data[:,0], data[:,3], data[:,4], fmt='-', ms=4, lw=1, alpha=0.6)\n",
    "#_ = plt.plot(data[:,0], data[:,3])\n",
    "plt.xlabel('Time (s)')\n",
    "_ = plt.ylabel('Counts/sec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-angel",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "- Let's split the light curve into two, and fit them simultaneously\n",
    "- For modeling the power spectrum (PSD), we use: `fqlag.Psd(...)`, which takes as input arrays of `time`, `countRate` and `countRateError`, in addition to an array defining the frequency bin boundaries.\n",
    "- The frequency bins need to incorporate all the frequencies available in the data from `fmin` to `fmax`, where `fmin=1/T`, with `T` bin the longest separation between points in the light curves, or `T = time[-1] - time[0]`, and `fmax = 1/(2*dt)`, where `dt` is the shortest separation, or `dt = min(time[1:] - time[:-1])`\n",
    "- We add two bins at the start and end of our frequency bins that our simulations show they reduce bias in the first and last bins. The psd (and delay later) will be discarded and used because they are highly biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the light curve into segments #\n",
    "isplit = data.shape[0]//2\n",
    "timeArrays = [data[:isplit,0], data[isplit:,0]]\n",
    "lc1Arrays  = [data[:isplit,1], data[isplit:,1]]\n",
    "lc1Errors  = [data[:isplit,2], data[isplit:,2]]\n",
    "lc2Arrays  = [data[:isplit,3], data[isplit:,3]]\n",
    "lc2Errors  = [data[:isplit,4], data[isplit:,4]]\n",
    "\n",
    "print(f'We have {len(timeArrays)} segments, with lengths ({len(timeArrays[0])}, {len(timeArrays[1])})')\n",
    "print('\\n', '-'*20)\n",
    "\n",
    "\n",
    "# frequency bins\n",
    "fmin = np.min([1./(timeArrays[0][-1] - timeArrays[0][0]), \n",
    "               1./(timeArrays[1][-1] - timeArrays[1][0])])\n",
    "fmax = np.max([0.5/np.min(timeArrays[0][1:] - timeArrays[0][:-1]), \n",
    "               0.5/np.min(timeArrays[1][1:] - timeArrays[1][:-1])])\n",
    "fqBins = np.logspace(np.log10(2*fmin), np.log10(0.5*fmax), 7)\n",
    "fqBins = np.concatenate([[0.1*fqBins[0]], fqBins, [2*fqBins[-1]] ])\n",
    "print('\\nThe frequency bins are:', ' '.join(['%4.4g'%x for x in fqBins]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-posting",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### Calculate the psd of the first light curve\n",
    "\n",
    "For each light curve we have, we create a model using `fqlag.Psd(...)`, and then create a model that calculates the joints likelihood using: `fqlag.multiFqLagBin`\n",
    "\n",
    "In a simple use, `fqlag.Psd(...)` takes three arrays as input: `time`, `countRate` and `countRateError`, plus the array defining frequency bins. We also, request that the model calculates the psd values in natural log units by passing `log=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the models for individual light curves\n",
    "psdMods = [fqlag.Psd(timeArrays[i], lc1Arrays[i], lc1Errors[i], fqBins, log=True) for i in range(2)]\n",
    "#psdMods = [fqlag.Psd(data[:,0], data[:,1], data[:,2], fqBins, log=True)]\n",
    "\n",
    "# merge the models that calculates the joints likelihood \n",
    "psdMod = fqlag.multiFqLagBin(psdMods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-vacuum",
   "metadata": {},
   "source": [
    "They only thing left before starting the likelihood maximization is a starting array holding the input psd values at the chosen frequency bins, this should have a length `len(fqBins)-1`. \n",
    "\n",
    "We then print the log likelihood at these input values as a test that things are ok.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial parameters, start with ones\n",
    "inPars = np.ones_like(fqBins[:-1])\n",
    "\n",
    "# print the loglikelihood for the input values ##\n",
    "logLike = psdMod.loglikelihood(inPars)\n",
    "\n",
    "print('Input parameters: ', ' '.join(['%4.4g'%x for x in inPars]))\n",
    "print('Corresponding Log-likelihood: %4.4g'%logLike)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-complexity",
   "metadata": {},
   "source": [
    "---\n",
    "### Maximizing the likelihood function\n",
    "\n",
    "We now do the likelihood maximization using `fqlag.misc.maximize`, which uses the `BFGS` method from `scipy.optimize`.\n",
    "\n",
    "It takes as arguments, the model we just defined, and the input array (plus other arguments to control the fit). \n",
    "This uses a quadratic approximation to the likelihood, and loops until a best fit value is found. The result is returned as a tuple of best fit values (`psd`), their ESTIMATED uncertainties (`psdErr`) taken from the hessian of the likelihood function and the fit obsject returned by `scipy.optimize.minimize`. The uncertainty estimates are generally a lower limit only. One needs to used either `fqlag.misc.errors` or `fqlag.misc.run_mcmc` to get full errors.\n",
    "\n",
    "The progress is printed:\n",
    "\n",
    "- Log-Likelihood | model-parameters | gmax: maximum absolute gradient at current location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximize the log-likelihood\n",
    "psd, psdErr, psdFit = fqlag.misc.maximize(psdMod, inPars)\n",
    "\n",
    "print('psd values:\\t\\t', '\\t'.join(['%4.4g'%x for x in psd]))\n",
    "print('psd uncertainties:\\t', '\\t'.join(['%4.4g'%x for x in psdErr]))\n",
    "print('Maximum Likelihood:\\t%4.4g'%(-psdFit.fun) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-coral",
   "metadata": {},
   "source": [
    "Note that the last value in the psd is hardly constrained, and it is consistent with zero. i.e. no variability power. This is because this band is dominated by the noise in the light curves.\n",
    "\n",
    "---\n",
    "### Plot the results\n",
    "We compare these estimates from the simple estimate from the square absolute of the Fourier transform of the raw light curve that we started with.\n",
    "\n",
    "Note that the raw light curve is plotted without subtracting the poisson noise, which is manifested as a constant at high frequencies. Our model fitting gives values that are noise-subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte the raw psd #\n",
    "n = data_raw.shape[0]\n",
    "freq_raw = np.fft.rfftfreq(n, 1.0)[1:-1]\n",
    "psd_raw  = (2*1.0/(n*data_raw[:,1].mean()**2)) * np.abs(np.fft.rfft(data_raw[:,1])[1:-1])**2\n",
    "\n",
    "\n",
    "# frequency from the geometric mean of our selected bins\n",
    "freq = 10**(np.log10( (fqBins[:-1]*fqBins[1:]) )/2.)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.errorbar(freq[1:-1], psd[1:-1], yerr=psdErr[1:-1], fmt='-o', ms=10, label='fit')\n",
    "plt.plot(freq_raw, np.log(psd_raw), label='raw psd (with noise)', alpha=0.5)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Log(PSD)')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-pontiac",
   "metadata": {},
   "source": [
    "---\n",
    "### Run MCMC chain to estimate the posterior distribution\n",
    "This uses the `emcee` package, so it should be installed (e.g. `pip install emcee`)\n",
    "\n",
    "The returned array has shape `(ntotal_runs, npar+1)`, where loglikelihood value is returned in the last element of the 2nd dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = fqlag.misc.run_mcmc(psdMod, psd, psdErr, nwalkers=20, nrun=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the results in a corner plot. Please install corner if not installed\n",
    "try:\n",
    "    import corner\n",
    "    # we use the last half of the chains\n",
    "    _ = corner.corner(chain[chain.shape[1]//2:,1:-2], smooth=1, smooth1d=1, \n",
    "                      labels=['P%d'%i for i in range(1,len(fqBins))])\n",
    "except ImportError:\n",
    "    print('Please install corner with \"pip install corner\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-springfield",
   "metadata": {},
   "source": [
    "---\n",
    "### Repeat the calculations for the psd of the second light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the models for individual light curves\n",
    "psdMods = [fqlag.Psd(timeArrays[i], lc2Arrays[i], lc2Errors[i], fqBins, log=True) for i in range(2)]\n",
    "\n",
    "# merge the models that calculates the joints likelihood \n",
    "psd2Mod = fqlag.multiFqLagBin(psdMods)\n",
    "\n",
    "# maximize the log-likelihood\n",
    "psd2, psd2Err, psd2Fit = fqlag.misc.maximize(psd2Mod, inPars)\n",
    "\n",
    "print('psd values:\\t\\t', '\\t'.join(['%4.4g'%x for x in psd2]))\n",
    "print('psd uncertainties:\\t', '\\t'.join(['%4.4g'%x for x in psd2Err]))\n",
    "print('Maximum Likelihood:\\t%4.4g'%(-psd2Fit.fun) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also plot the result of the second psd\n",
    "\n",
    "# calcualte the raw psd #\n",
    "n = data_raw.shape[0]\n",
    "freq2_raw = np.fft.rfftfreq(n, 1.0)[1:-1]\n",
    "psd2_raw  = (2*1.0/(n*data_raw[:,3].mean()**2)) * np.abs(np.fft.rfft(data_raw[:,3])[1:-1])**2\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.errorbar(freq[1:-1], psd2[1:-1], yerr=psd2Err[1:-1], fmt='-o', ms=10, label='fit-2')\n",
    "plt.errorbar(freq[1:-1], psd[1:-1], yerr=psdErr[1:-1], fmt='-o', ms=10, label='fit-1')\n",
    "plt.plot(freq2_raw, np.log(psd2_raw), label='raw psd2 (with noise)', alpha=0.5)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Log(PSD)')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-stress",
   "metadata": {},
   "source": [
    "---\n",
    "### Calculate the cross spectrum and delay\n",
    "\n",
    "- The PSD values from above will be used as input for the modeling\n",
    "- The model calculates the *phase* $\\phi$ delay not the time delay $\\tau$. These are related by: $\\tau(f) = \\phi(f)/2\\pi f$, where $f$ is the frequency at which the delay is measured (i.e. the center of the bin)\n",
    "- The procedure is similar to the PSD case, but now the model taken both light curves as input. For instance, whereas in the PSD case the time array was given as a single numby array, here, a list that contains the first and the second time arrays should passed to `fqlag.Cxd`.\n",
    "- Our simulated light curves have an input phase delay of $\\phi=1.0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the models for individual light curves\n",
    "cxdMods = [fqlag.Cxd([timeArrays[i], timeArrays[i]], \n",
    "                     [lc1Arrays[i], lc2Arrays[i]], \n",
    "                     [lc1Errors[i], lc2Errors[i]], fqBins, psd, psd2, log=True) for i in range(2)]\n",
    "\n",
    "# merge the models that calculates the joints likelihood \n",
    "cxdMod = fqlag.multiFqLagBin(cxdMods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-navigator",
   "metadata": {},
   "source": [
    "We define the input parameters similar to before, but now, a number of bins of `nfq`, we have `2*nfq` model parameters, `nfq` for the cross spectral values, and `nfq` for the phase values.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial parameters, start with ones\n",
    "inCxd = .5*(psd + psd2) - 1\n",
    "inPars = np.concatenate([inCxd, inCxd*0])\n",
    "\n",
    "# print the loglikelihood for the input values ##\n",
    "logLike = cxdMod.loglikelihood(inPars)\n",
    "\n",
    "print('Input parameters: ', ' '.join(['%4.4g'%x for x in inPars]))\n",
    "print('Corresponding Log-likelihood: %4.4g'%logLike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cxd, cxdErr, cxdFit = fqlag.misc.maximize(cxdMod, inPars)\n",
    "cxd, cxdErr, cxdFit = fqlag.misc.maximize_no_grad(cxdMod, inPars)\n",
    "\n",
    "print('psd values:\\t\\t', '\\t'.join(['%4.4g'%x for x in cxd]))\n",
    "print('psd uncertainties:\\t', '\\t'.join(['%4.4g'%x for x in cxdErr]))\n",
    "print('Maximum Likelihood:\\t%4.4g'%(-cxdFit.fun) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-consortium",
   "metadata": {},
   "source": [
    "---\n",
    "### Plot the result of the delay calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result of the cross spectrum calculations\n",
    "\n",
    "# calcualte the raw psd #\n",
    "n = data_raw.shape[0]\n",
    "freq_raw = np.fft.rfftfreq(n, 1.0)[1:-1]\n",
    "phi_raw  = np.angle(np.fft.rfft(data_raw[:,1])[1:-1] * np.conj(np.fft.rfft(data_raw[:,3])[1:-1]))\n",
    "\n",
    "\n",
    "Cxd, Phi = np.split(cxd, 2)\n",
    "CxdErr, PhiErr = np.split(cxdErr, 2)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.errorbar(freq[1:-1], Phi[1:-1], yerr=PhiErr[1:-1], fmt='-o', ms=10, label='fit')\n",
    "plt.plot(freq_raw, phi_raw, label='raw phase lag', alpha=0.5)\n",
    "plt.plot(freq_raw, freq_raw*0 + 1, label='input phase lag')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Phase Lag')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-commission",
   "metadata": {},
   "source": [
    "---\n",
    "### Run MCMC chain to estimate the posterior distribution\n",
    "This uses the `emcee` package, similar to the psd case above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "cxdChain = fqlag.misc.run_mcmc(cxdMod, cxd, cxdErr, nwalkers=-4, nrun=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the results in a corner plot. Please install corner if not installed\n",
    "try:\n",
    "    import corner\n",
    "    # we use the last half of the chains\n",
    "    _ = corner.corner(cxdChain[chain.shape[1]//2:,(len(fqBins)+1):-2], smooth=1, smooth1d=1, \n",
    "                      labels=['$\\phi_%d$'%i for i in range(1,len(fqBins))],\n",
    "                     truths=np.ones(len(fqBins)-2))\n",
    "except ImportError:\n",
    "    print('Please install corner with \"pip install corner\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-selling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
